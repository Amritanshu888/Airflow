{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c4a1a4",
   "metadata": {},
   "source": [
    "- To install Astro : winget install -e --id Astronomer.Astro\n",
    "- astro dev init ---> Automatically it will initialize a astro project pulling the airflow development files from astro runtime 12.1.1\n",
    "- It is going to initialize a empty astro project in this drive.\n",
    "- astro dev init ---> It will initialize a project structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd29f8",
   "metadata": {},
   "source": [
    "- In project structure we have dags folder --> which is basically for airflow.\n",
    "- In project structure a Dockerfile is also created , once we run the entire project this will be the docker file which will run inside a docker container. --> This innternally also calls airflow and it will be running in one another port itself.\n",
    "- Inside the dags folder only we basically create all our dags ---> an example file will already be there with name exampledag.py\n",
    "- exampledag.py is a default example given by astro. Its a ETL example. Its getting astronaut details from the API , after getting info its returning the list of people in the space , then another task which is printing all the names. 2 specific tasks have been scheduled , when the api has new data we should be able to schedule it regularly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214a375",
   "metadata": {},
   "source": [
    "## Run\n",
    "- To run the project see in docker nothing should be running , docker should be installed and opened.\n",
    "- command : astro dev start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487faa7b",
   "metadata": {},
   "source": [
    "- It will start building from here , its building the docker file , its building the docker image , its going to run each and everything in the docker itself.\n",
    "- Now when u go to docker desktop u will see that ur containers have got started.\n",
    "- If the Airflow UI doesn't gets opened automatically , u can go to docker and from there u can access web server.\n",
    "- This is the Airflow UI , completely managed by Astro and this entirely is running in a docker container.\n",
    "- In Airflow UI u will be able to see ur Dags getting created , right now there will be only one DAG i.e. example_astronauts , initially it will be disabled bocz u have not enabled.\n",
    "- Go inside the DAG by double clicking it , there u will be able to see in this DAG i have two important tasks , 1. get_astronauts 2. print_astronaut_craft. When u double-click(also enable it) and get into it u will be able to see multiple things from the UI itself : Graph format, Gannt chart , code , run duration , task duration , calender ,event log ,details.\n",
    "- In the UI u can see schedule which is set to daily , u also have the option to trigger this dag (available in UI basically ).\n",
    "- Trigger is basically to run the DAG.\n",
    "- Next run id will be there : Time at which it has to run next.\n",
    "- Latest runid will be there : Time at which it ran previously.\n",
    "- U can trigger it manually by clicking on the Trigger button. ---> Now both the tasks will be running.\n",
    "- Go to graph after u press trigger , if the graph(both tasks in graph) is giving green signal then its basically running.\n",
    "- After running go to get_astronauts in the DAG , in that u go to Event log u will be able to see previous logs , in code u can see Code.\n",
    "- Logs will be basically for entire DAG. Code also u will be able to see.\n",
    "- In example_astronauts there will be something called as Event/Audit log . All previous logs will be there.\n",
    "- In UI we will also have something called as XCom ----> Which is basically what information is being passed from one task to the other task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b04c7",
   "metadata": {},
   "source": [
    "## Stop this:\n",
    "- command : astro dev stop -----> after this my entire docker container will get stopped.\n",
    "- Hence my entire application will also get stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7bdeb",
   "metadata": {},
   "source": [
    "- In the project folder inside tests folder u have to write test cases.(By default for the exampledag.py test cases has been written)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728341f",
   "metadata": {},
   "source": [
    "- .env ---> if u want to write environment variables.\n",
    "- airflow_settings.yaml ---> if u want to write settings over there.\n",
    "- requirements.txt ----> if u want to install any libraries.\n",
    "- README.md ---> To see the written info.\n",
    "- All this is present in the project structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ba41e",
   "metadata": {},
   "source": [
    "## Note:\n",
    "- We create futher dags(tasks in that dag) in the dag folder only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9b583",
   "metadata": {},
   "source": [
    "## Task API\n",
    "- Airflow has a feature which is called as Task Flow API , which actually simplifies ur entire task creation process inside ur DAG , along with that u don't even have to go and create context , which we created with the help of xcom."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
