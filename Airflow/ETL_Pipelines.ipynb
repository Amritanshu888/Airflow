{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e4d73ca",
   "metadata": {},
   "source": [
    "## ETL Pipelines\n",
    "- There are 3 key terms:\n",
    "- 1. Extract \n",
    "- 2. Transform\n",
    "- 3. Load\n",
    "\n",
    "- Big data engineering team goes and follows this ETL pipeline process. This ETL pipeline is the part of the data pipeline.\n",
    "- Suppose we have a source from where we have to take data for our problem statement this source can be API's , Internal Database , IOT devices , similarily there can be various sources.\n",
    "- What data engineering person will do ??\n",
    "- We need to go ahead and create something called as data pipeline. In this data pipeline our main task is to integrate or combine all these data sources(sources mentioned above) together and then do some kind of preprocessing and transformation on them.\n",
    "- In preprocessing and transformation what i can do as data scientist is that (we have combined all the data) then we convert it into a json format which will have the entire data together. So this part where we are basically taking data from the source is called as extract.\n",
    "- When we are doing preprocessing and transformation its basically transform phase. Cleaning of the data and then convert it into a format (json) is basically Transform phase.\n",
    "- Why converting to json ??\n",
    "- All these combined data will be stored in one specific source , and when further going ahead the data scientists will be dependent on this source only. They will not be dependent on multiple sources available over here.\n",
    "- We will take this json and store it in some kind of source.(This is loading phase as here we are loading the data in some source).\n",
    "- Transformation we have done by converting it into json and then loading it in one particular source is the load phase.\n",
    "- This source can be ur SQL server , MongoDB server , it can also be ur Postgres Server or NoSQL data (anything upto u).\n",
    "- Then data scientists need to be only dependent on this one source as all this data is basically getting combined and then stored in one source.\n",
    "- This is the reason we say this as ETL(Extract,Transform,Load). This ETL we say this as Data Pipeline bcoz its collecting data from various sources , combining them , and then loading them into a specific source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2153890",
   "metadata": {},
   "source": [
    "- Airflow is a amazing open source platform which will actually help u to create this amazing data pipelines which includes this extract , transform , load.\n",
    "- In projects we will take the data from an API (basically read the data from an API) ---> then transform it into json format ----> then will save this in some kind of database(the database we will use here is postgres SQL).\n",
    "- We will run this database in docker container. ---> Will give us an idea that in airflow how can we communicate from one container to the other container.\n",
    "- Why using Airflow ??\n",
    "- Bcoz the process mentioned above in projects needs to be completely scheduled , bcoz for my problem statement i need to get data every week/day. Inorder to schedule in this way we have to use something called as Airflow.\n",
    "- Airflow is basically helping us in scheduling our ETL pipeline.\n",
    "- We will run everything in docker container as this will allow it to be independent while deploying."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
